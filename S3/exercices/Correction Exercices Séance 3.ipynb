{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "76178dff-9423-4160-9bc5-7e58fee7afe9",
   "metadata": {},
   "source": [
    "# Exercices Séance 3"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "db304128",
   "metadata": {},
   "source": [
    "## Exercice 1 : écrire des fichiers\n",
    "\n",
    "Faire le script complet permettant de récupérer les pages webs suivantes, et les stocker dans un dossier \"pages\" "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "398b74f8",
   "metadata": {},
   "outputs": [],
   "source": [
    "urls = [\n",
    "    \"https://www.unil.ch/unil/fr/home.html\",\n",
    "    \"http://cuso.ch\",\n",
    "    \"https://www.unige.ch/\",\n",
    "    \"https://www.u-bordeaux.fr/\",\n",
    "]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bd69756d",
   "metadata": {},
   "source": [
    "Je veux donc :\n",
    "\n",
    "Pour chaque adresse (donc sans doute une boucle) :\n",
    "- faire une requête sur l'url\n",
    "- récupérer son contenu\n",
    "- l'écrire dans un fichier\n",
    "- lui même contenu dans un dossier \"pages\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "a604cf8e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "import os  # va nous aider à créer/écrire des choses sur notre ordi\n",
    "\n",
    "# Créer le dossier \"pages\" si il n'existe pas\n",
    "# on aurait pu le faire à la main, mais ici on automatise\n",
    "os.makedirs(\"pages\", exist_ok=True)\n",
    "\n",
    "for url in urls:\n",
    "    response = requests.get(url)\n",
    "\n",
    "    # dark, j'explique dessous\n",
    "    filename = url.replace(\"/\", \"_\") + \".txt\"\n",
    "\n",
    "    # Utiliser le module os pour donner un chemin et nom de fichier\n",
    "    with open(os.path.join(\"pages\", filename), \"w\") as f:\n",
    "        f.write(response.text)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6dcb890a",
   "metadata": {},
   "source": [
    "**Pourquoi le `.replace` ?**\n",
    "\n",
    "si je garde les `/` ça foire car c'est aussi comme ça que l'on indique un chemin de dossier.  \n",
    "Je remplace donc par un truc qui pose pas de pb dans un nom de fichier : \"`_`\".  \n",
    "C'est moche, c'est nul, mais ça marche ici."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ed2c893b",
   "metadata": {},
   "source": [
    "**Autre possibilité,** plus stable, slugify : https://pypi.org/project/python-slugify/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "da2ff9f4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# %pip install python-slugify"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f62956b2",
   "metadata": {},
   "outputs": [],
   "source": [
    "from slugify import slugify\n",
    "\n",
    "slug = slugify(\"https://www.unil.ch/unil/fr/home.html\")\n",
    "slug"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "38a73c2f",
   "metadata": {},
   "source": [
    "### \"faites-mieux\"  (JLM vibe)\n",
    "Il y a toujours mieux… mais ça marche.  \n",
    "Ici en réalité il faudrait gérer les exceptions, etc.  \n",
    "Mais voilà.  \n",
    "\"it works\"\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e5a2a996",
   "metadata": {},
   "source": [
    "----"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cc5d9acc",
   "metadata": {},
   "source": [
    "## Exercice 2 : utiliser beautiful soup\n",
    "\n",
    "- Installer la bibliothèque beautiful soup https://www.crummy.com/software/BeautifulSoup/\n",
    "- Récupérer tous les liens de la page cuso avec la méthode find_all (un lien est une balise a)\n",
    "- Ecrire dans un fichier tous les liens"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a5c66fa1",
   "metadata": {},
   "source": [
    "**read the doc !**  \n",
    "Regarder la doc : https://beautiful-soup-4.readthedocs.io/en/latest/#quick-start"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6218875e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# %pip install beautifulsoup4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8ce136cd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# version \"brute\" de base\n",
    "from bs4 import BeautifulSoup\n",
    "import requests\n",
    "\n",
    "# Récupérer la page\n",
    "response = requests.get(\"http://cuso.ch\")\n",
    "soup = BeautifulSoup(response.content, \"html.parser\")\n",
    "\n",
    "# Trouver toutes les balises <a>\n",
    "liens = soup.find_all(\"a\")\n",
    "\n",
    "# Enregistrer ce truc moche dans un fichier\n",
    "with open(\"liens_moches_cuso.txt\", \"w\") as f:\n",
    "    f.write(str(liens))  # ici je transforme en str (sinon le type ResultSet passe pas)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "51419af4",
   "metadata": {},
   "source": [
    "### Fini ? (1)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4ab9f41d",
   "metadata": {},
   "source": [
    "### READ THE DOC !!!!!!\n",
    "\n",
    "> One common task is extracting all the URLs found within a page’s <a> tags:\n",
    ">```python\n",
    ">for link in soup.find_all('a'):\n",
    ">    print(link.get('href'))\n",
    ">```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "480dd3b4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# En fait nos \"liens\" sont pas des liens\n",
    "# ie c'est moche\n",
    "liens[5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "29269fe6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# on peut choper ce qui nous intéresse plus ici :\n",
    "liens[5].get(\"href\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b28c0f21",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Améliorer notre code :\n",
    "\n",
    "# Extraire les URLs (href)\n",
    "for lien in liens:\n",
    "    href = lien.get(\"href\")\n",
    "    print(href)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1af8d9ec",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Fusionner les logiques :\n",
    "# améliorer notre code de base et renvoyer le tout dans la sortie fichier\n",
    "\n",
    "# importer\n",
    "from bs4 import BeautifulSoup\n",
    "import requests\n",
    "\n",
    "# Récupérer la page\n",
    "response = requests.get(\"http://cuso.ch\")\n",
    "soup = BeautifulSoup(response.content, \"html.parser\")\n",
    "\n",
    "# Trouver toutes les balises <a>\n",
    "liens = soup.find_all(\"a\")\n",
    "\n",
    "#######################\n",
    "# ON MODIFIE ci-dessous\n",
    "#######################\n",
    "\n",
    "# Extraire les href des liens et les écrire dans un fichier\n",
    "with open(\"liens_cuso.txt\", \"w\") as f:\n",
    "    for lien in liens:\n",
    "        href = lien.get(\"href\")  # cette fois je récupère bien le href\n",
    "        f.write(href + \"\\n\")  # je saute une ligne entre les éléments avec \"\\n\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5b871903",
   "metadata": {},
   "source": [
    "----"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d857f4e2",
   "metadata": {},
   "source": [
    "### Fini ?? (2) (possiblement pour plus tard selon le timing)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "33ad842e",
   "metadata": {},
   "outputs": [],
   "source": [
    "liens[0].get(\"href\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "62c0c1f4",
   "metadata": {},
   "source": [
    "**kekoikest-ce ?**\n",
    "\n",
    "auto référence : des liens qui ne sont pas des liens externes pointant vers d'autres sites, mais propres au site de la cuso.    \n",
    "et l'idée c'est :  \n",
    "url_de_base + suite_url (/ ; /en/ ; etc.)  \n",
    "\n",
    "COOL !! je sais concaténer des chaines de caractères :  \n",
    "\n",
    "ma_chaine + la_suite"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "667e4b49",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"http://cuso.ch\" + \"/en/\"  # et je suis sur la version anglaise"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3768e3f9",
   "metadata": {},
   "source": [
    "COOL !\n",
    "\n",
    "COOL ???? \n",
    "\n",
    "…………………\n",
    "\n",
    "PAS COOL !!!!!!! : je ne veux pas ajouter \"http://cuso.ch\" devant les liens externes :\n",
    "\n",
    "\"http://cuso.ch\" + \"https://www.unifr.ch/\" = cata\n",
    "\n",
    "MAIS y a une solution pour ça : la fonction `urljoin` de urllib.parse"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b9a8ac49",
   "metadata": {},
   "outputs": [],
   "source": [
    "from urllib.parse import urljoin\n",
    "\n",
    "for lien in liens:\n",
    "    href = lien.get(\"href\")\n",
    "    url_complet = urljoin(\"http://cuso.ch\", href)\n",
    "    print(url_complet)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7279654e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# On implémente pour avoir des vraies url\n",
    "\n",
    "# importer\n",
    "from bs4 import BeautifulSoup\n",
    "import requests\n",
    "from urllib.parse import urljoin\n",
    "\n",
    "# Récupérer la page\n",
    "response = requests.get(\"http://cuso.ch\")\n",
    "soup = BeautifulSoup(response.content, \"html.parser\")\n",
    "\n",
    "# Trouver toutes les balises <a>\n",
    "liens = soup.find_all(\"a\")\n",
    "\n",
    "# Extraire les URL des liens et les écrire dans un fichier\n",
    "with open(\"urls_cuso.txt\", \"w\") as f:\n",
    "    for lien in liens:\n",
    "        href = lien.get(\"href\")\n",
    "        url_complet = urljoin(\"http://cuso.ch\", href)  # on peauffine les url\n",
    "        f.write(url_complet + \"\\n\")  # je saute une ligne entre les élesments avec \"\\n\"\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "110c6917",
   "metadata": {},
   "source": [
    "### Fini ??? (3) - pour plus tard\n",
    "\n",
    "Si vous voulez aller plus loin en dehors de la correction de la séance :\n",
    "\n",
    "Bah on peut toujours améliorer…"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8412756c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# AUTRE LOGIQUE\n",
    "\n",
    "# On aurait pu plutôt vouloir mettre tous les liens dans une liste\n",
    "# Puis les sortir dans le fichier ensuite\n",
    "# = une fois que j'ai ma liste je peux continuer a faire des opérations dessus\n",
    "\n",
    "# importer\n",
    "from bs4 import BeautifulSoup\n",
    "import requests\n",
    "\n",
    "# Récupérer la page\n",
    "response = requests.get(\"http://cuso.ch\")\n",
    "soup = BeautifulSoup(response.content, \"html.parser\")\n",
    "\n",
    "# Trouver toutes les balises <a>\n",
    "liens = soup.find_all(\"a\")\n",
    "\n",
    "# Créer une liste contenant toutes les urls\n",
    "liste_urls = []\n",
    "for lien in liens:\n",
    "    href = lien.get(\"href\")  # on récupère le href\n",
    "    url_complet = urljoin(\"http://cuso.ch\", href)  # on peauffine les url\n",
    "    liste_urls.append(url_complet)  # on les ajoute à notre liste\n",
    "\n",
    "# Enregistrer la liste dans un fichier (un lien par ligne)\n",
    "with open(\"toujours_plus_de_solutions.txt\", \"w\", encoding=\"utf-8\") as f:\n",
    "    for u in liste_urls:\n",
    "        f.write(u + \"\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6d3ebc0d",
   "metadata": {},
   "outputs": [],
   "source": [
    "### Puis améliorer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e7dffd0e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# importer\n",
    "from bs4 import BeautifulSoup\n",
    "import requests\n",
    "from urllib.parse import urljoin\n",
    "\n",
    "# Récupérer la page\n",
    "response = requests.get(\"http://cuso.ch\")\n",
    "soup = BeautifulSoup(response.content, \"html.parser\")\n",
    "\n",
    "# Trouver toutes les balises <a>\n",
    "liens = soup.find_all(\"a\")\n",
    "\n",
    "# Créer une liste contenant toutes les urls avec une list comprehension\n",
    "liste_urls = [urljoin(\"http://cuso.ch\", lien.get(\"href\")) for lien in liens]\n",
    "\n",
    "# Enregistrer la liste dans un fichier (un lien par ligne)\n",
    "with open(\"encore_plus_de_solutions.txt\", \"w\", encoding=\"utf-8\") as f:\n",
    "    f.write(\"\\n\".join(liste_urls))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "781513d6",
   "metadata": {},
   "source": [
    "Et on gère toujours pas les erreurs possibles…\n",
    "\n",
    "etc."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "myenv_clone",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
